{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4012a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:prophet.plot:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from analysis.load_data import load_dataset\n",
    "from analysis.config.model_config import FORECASTING_CONFIG\n",
    "from analysis.utils.preprocessing import prepare_time_series\n",
    "from analysis.forecasting.forecasting_utils import train_prophet_model, hyperparameter_search\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b92c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "/Users/Andrew/Desktop/Computer Science/Mental_Health_Project/analysis/load_data.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "INFO:analysis.load_data:Loaded 4 rows from news_extract.sql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Length:  4\n",
      "Columns: ['DATE', 'ARTICLE_COUNT', 'SAMPLE_HEADLINES', 'SOURCES']\n",
      "\n",
      "First few rows:\n",
      "         DATE  ARTICLE_COUNT  \\\n",
      "0  2025-10-22             93   \n",
      "1  2025-10-23             92   \n",
      "2  2025-10-24             32   \n",
      "3  2025-10-25             97   \n",
      "\n",
      "                                    SAMPLE_HEADLINES  \\\n",
      "0  Mexico to Tax Mature Video Games | Pregnant IC...   \n",
      "1  Villains To Be Removed at Disney Park: Will Ma...   \n",
      "2  Psychologist Warns of Donald Trump’s ‘Massive ...   \n",
      "3  Particulate matter pollutant levels cross Indi...   \n",
      "\n",
      "                                             SOURCES  \n",
      "0  NBC News, Landezine.com, Catholicnewsagency.co...  \n",
      "1  Yahoo Entertainment, TheStranger.com, Rolling ...  \n",
      "2  Bemorewithless.com, SFGate, New York Post, For...  \n",
      "3  ABC News (AU), Daily Signal, XDA Developers, A...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "/Users/Andrew/Desktop/Computer Science/Mental_Health_Project/analysis/load_data.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "INFO:analysis.load_data:Loaded 5484 rows from suicide_demographics_extract.sql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suicide Demographics Length:  5484\n",
      "Columns: ['indicator', 'unit', 'stub_name', 'stub_label', 'year', 'age', 'estimate', 'flag', 'demographic_category', 'demographic_value']\n",
      "\n",
      "First few rows:\n",
      "                 indicator                                               unit  \\\n",
      "0  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "1  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "2  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "3  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "4  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "\n",
      "  stub_name   stub_label  year       age  estimate  flag demographic_category  \\\n",
      "0     Total  All persons  1950  All ages      13.2  None                Total   \n",
      "1     Total  All persons  1960  All ages      12.5  None                Total   \n",
      "2     Total  All persons  1970  All ages      13.1  None                Total   \n",
      "3     Total  All persons  1980  All ages      12.2  None                Total   \n",
      "4     Total  All persons  1981  All ages      12.3  None                Total   \n",
      "\n",
      "  demographic_value  \n",
      "0       All persons  \n",
      "1       All persons  \n",
      "2       All persons  \n",
      "3       All persons  \n",
      "4       All persons  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "/Users/Andrew/Desktop/Computer Science/Mental_Health_Project/analysis/load_data.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "INFO:analysis.load_data:Loaded 38316 rows from who_suicide_extract.sql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WHO Suicide Length:  38316\n",
      "Columns: ['country', 'year', 'sex', 'age', 'suicides_no', 'population', 'suicide_rate_per_100k']\n",
      "\n",
      "First few rows:\n",
      "   country  year     sex          age  suicides_no  population  \\\n",
      "0  Albania  1985  Female  15-24 years          0.0    277900.0   \n",
      "1  Albania  1985  Female  25-34 years          0.0    246800.0   \n",
      "2  Albania  1985  Female  35-54 years          0.0    267500.0   \n",
      "3  Albania  1985  Female   5-14 years          0.0    298300.0   \n",
      "4  Albania  1985  Female  55-74 years          0.0    138700.0   \n",
      "\n",
      "   suicide_rate_per_100k  \n",
      "0                    0.0  \n",
      "1                    0.0  \n",
      "2                    0.0  \n",
      "3                    0.0  \n",
      "4                    0.0  \n"
     ]
    }
   ],
   "source": [
    "# Load News data\n",
    "news_df = load_dataset('news')\n",
    "print(\"News Length: \", len(news_df))\n",
    "print(f\"Columns: {list(news_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(news_df.head())\n",
    "\n",
    "# Load Redit Data\n",
    "reddit_df = load_dataset('reddit')\n",
    "print(\"Reddit Length: \", len(reddit_df))\n",
    "print(f\"Columns: {list(reddit_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(reddit_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b463a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare CDC anxiety data\n",
    "cdc_ts = prepare_time_series(cdc_df, 'date', 'anxiety')\n",
    "\n",
    "# Prepare Google Trends data\n",
    "trends_ts = prepare_time_series(trends_df, 'date', 'interest')\n",
    "\n",
    "\n",
    "# Convert all to Prophet format\n",
    "datasets_to_forecast = {}\n",
    "\n",
    "# CDC data\n",
    "cdc_prophet = cdc_ts.reset_index()\n",
    "cdc_prophet.columns = ['ds', 'y']\n",
    "datasets_to_forecast['cdc_anxiety'] = cdc_prophet\n",
    "\n",
    "# Google Trends data\n",
    "trends_prophet = trends_ts.reset_index()\n",
    "trends_prophet.columns = ['ds', 'y']\n",
    "datasets_to_forecast['google_trends'] = trends_prophet\n",
    "\n",
    "# WHO data\n",
    "who_prophet = who_suicide_ts.reset_index()\n",
    "who_prophet.columns = ['ds', 'y']\n",
    "datasets_to_forecast['who_suicides'] = who_prophet\n",
    "\n",
    "# Plot all time series for comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for i, (name, data) in enumerate(datasets_to_forecast.items()):\n",
    "    axes[i].plot(data['ds'], data['y'])\n",
    "    axes[i].set_title(f'{name.replace(\"_\", \" \").title()} Over Time')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Prophet model\n",
    "config = FORECASTING_CONFIG['prophet']\n",
    "forecast_results = {}\n",
    "\n",
    "for dataset_name, prophet_data in datasets_to_forecast.items():\n",
    "    # Train model using utility function\n",
    "    model = train_prophet_model(prophet_data, config)\n",
    "    \n",
    "    # Generate forecast (different periods based on data frequency)\n",
    "    if dataset_name == 'who_suicides':\n",
    "        periods = 5  # 5 years ahead for annual data\n",
    "    else:\n",
    "        periods = 90  # 90 days ahead for higher frequency data\n",
    "    \n",
    "    future = model.make_future_dataframe(periods=periods)\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Store results\n",
    "    forecast_results[dataset_name] = {\n",
    "        'model': model,\n",
    "        'forecast': forecast,\n",
    "        'training_data': prophet_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5417ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "if len(weekly_reddit) >= 20:  # Only tune if you have enough data\n",
    "    param_grid = {\n",
    "        'changepoint_prior_scale': [0.01, 0.05, 0.1],\n",
    "        'seasonality_prior_scale': [1.0, 10.0, 20.0]\n",
    "    }\n",
    "    best_params = hyperparameter_search(weekly_reddit, 'volume', param_grid, 'Reddit Volume')\n",
    "    # Merge with base config\n",
    "    tuned_config = {**FORECASTING_CONFIG['prophet'], **best_params}\n",
    "else:\n",
    "    tuned_config = FORECASTING_CONFIG['prophet']\n",
    "\n",
    "# Train with tuned or default config\n",
    "reddit_volume_model, reddit_volume_data = train_prophet_model(\n",
    "    df=weekly_reddit,\n",
    "    value_col='volume',\n",
    "    model_name='Reddit Volume',\n",
    "    config=tuned_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all forecasts\n",
    "fig, axes = plt.subplots(len(forecast_results), 2, figsize=(20, 6*len(forecast_results)))\n",
    "\n",
    "for i, (dataset_name, results) in enumerate(forecast_results.items()):\n",
    "    model = results['model']\n",
    "    forecast = results['forecast']\n",
    "    \n",
    "    # Main forecast plot\n",
    "    fig1 = model.plot(forecast, ax=axes[i,0])\n",
    "    axes[i,0].set_title(f'{dataset_name.replace(\"_\", \" \").title()} Forecast')\n",
    "    axes[i,0].set_ylabel('Value')\n",
    "    \n",
    "    # Components plot\n",
    "    fig2 = model.plot_components(forecast, ax=axes[i,1] if len(forecast_results) == 1 else None)\n",
    "    if len(forecast_results) > 1:\n",
    "        # For multiple subplots, create separate components plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        model.plot_components(forecast)\n",
    "        plt.suptitle(f'{dataset_name.replace(\"_\", \" \").title()} Components')\n",
    "        plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d969558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "evaluation_results = {}\n",
    "\n",
    "for dataset_name, results in forecast_results.items():\n",
    "    training_data = results['training_data']\n",
    "    \n",
    "    if len(training_data) > 30:\n",
    "        # Split data for validation\n",
    "        train_size = int(len(training_data) * 0.8)\n",
    "        train_data = training_data[:train_size]\n",
    "        test_data = training_data[train_size:]\n",
    "        \n",
    "        # Train evaluation model\n",
    "        eval_model = train_prophet_model(train_data, config)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        metrics = evaluate_forecast_model(eval_model, train_data, test_data, model_type='prophet')\n",
    "        evaluation_results[dataset_name] = metrics\n",
    "        \n",
    "        print(\"Model Evaluation Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric.upper()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"Insufficient data for evaluation ({len(training_data)} points)\")\n",
    "        evaluation_results[dataset_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models/results\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "for dataset_name, results in forecast_results.items():\n",
    "    model = results['model'] \n",
    "    forecast = results['forecast']\n",
    "    \n",
    "    filename = f'{dataset_name}_{timestamp}'\n",
    "    save_forecast_results(model, forecast, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_results = {}\n",
    "\n",
    "for dataset_name, results in forecast_results.items():\n",
    "    training_data = results['training_data']\n",
    "\n",
    "    # Adjust minimum data requirements based on frequency\n",
    "    min_required = 365 if dataset_name != 'who_suicides' else 15  # 15 years for annual data\n",
    "    \n",
    "    if len(training_data) > min_required:\n",
    "        try:\n",
    "            cv_output, cv_metrics = cross_validate_timeseries(training_data, config)\n",
    "            cv_results[dataset_name] = cv_metrics\n",
    "            \n",
    "            print(\"Cross-validation metrics summary:\")\n",
    "            print(cv_metrics[['mape', 'rmse']].describe())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Cross-validation failed: {e}\")\n",
    "            cv_results[dataset_name] = None\n",
    "    else:\n",
    "        print(f\"Need more data for cross-validation ({len(training_data)}/{min_required} points)\")\n",
    "        cv_results[dataset_name] = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
